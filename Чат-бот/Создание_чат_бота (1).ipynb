{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbYC4MOwYaBM"
      },
      "source": [
        "# Практическое задание: \"Создание чат-бота\"\n",
        "\n",
        "**Выполнил:** Новиков Павел  \n",
        "**Группа:** DSPR-73  \n",
        "**Описание:** Создать чат-бот, который по продуктовому запросу рекомендует товары, по остальным запросам отвечает \"болталкой\".  \n",
        "**Данные:** Продуктовые запросы находятся в файле  [ProductsDataset.zip](https://drive.google.com/file/d/17OdOPIH9d-T6d1Rb6F0IQuI7P9xoGWHi/view?usp=drive_link)    \n",
        "Основой для вопросов и ответов являются данные об ответах и вопросах с соответствующего раздела сайта mail.ru [Otvety.txt](https://disk.yandex.ru/d/ikeJ8Tu2LxPkeA)  \n",
        "**ВНИМАНИЕ!:** ЗАКОММЕЧЕННЫЕ СТРОКИ КОДА СОДЕРЖАТ ДОЛГОВЫПОЛНЯЮЩИЙСЯ КОД. ДЛЯ ЭКОНОМИИ ВРЕМЕНИ РЕЗУЛЬТАТЫ ВЫПОЛНЕНИЯ КОДА В ДАННЫХ ЯЧЕЙКАХ ЗАГРУЖАЮТСЯ В НАЧАЛЕ РАБОТЫ НОУТБУКА.  \n",
        "**Задачи:**\n",
        "*   Совершить предобработку текстов в данных об вопросах-ответах и продуктах:\n",
        "  *   Обучить **word2vec**\n",
        "  *   Получить эмбеддинги\n",
        "  *   Удалить знаки препинания\n",
        "  *   Сделать лемматизацию\n",
        "* Сложить в индекс все вопросы при помощи библиотеки **annoy**\n",
        "* Векторизовать ответы, вектором считать усреднённую сумму **word2vec** слов, которые входят в ответ  \n",
        "* Реализовать метод, который получит на вход вопрос и найдёт ответ к нему.\n",
        "* Обучить классификатор: продуктовый запрос vs. всё остальное (продуктовым можно считать запрос, который равен названию или описанию товара).\n",
        "* Добавить логику поиска похожих товаров по продуктовому запросу. Всю логику завернуть в метод get_answer(). Ответ на продуктовый запрос должен иметь вид \"product_id title\".\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIwLLzksjgaI",
        "outputId": "d2a2cf17-c79b-4fc2-c855-4125bfbcdcbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "cadaver is already the newest version (0.23.3-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 18 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "! pip install annoy --quiet\n",
        "! pip install stop-words --quiet\n",
        "! pip install pymorphy2 --quiet\n",
        "! sudo apt-get install cadaver --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5b2920f8-5661-4b82-9a78-573cc78132f5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# методы работы со строками\n",
        "import string\n",
        "# Алгоритм классификации по методу поиска ближайших соседей(ANN) в Python,\n",
        "# оптимизированный для использования памяти и загрузки/сохранения на диск.\n",
        "import annoy\n",
        "# библиотека для раскодирования файлов\n",
        "import codecs\n",
        "\n",
        "from pymorphy2 import MorphAnalyzer\n",
        "from stop_words import get_stop_words\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# выводит индикатор выполнения итератора\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import time\n",
        "\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import metrics\n",
        "\n",
        "from annoy import AnnoyIndex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTHMwO0Jd6XC"
      },
      "outputs": [],
      "source": [
        "#Загрузим файл в Google Colab из Яндекс WebDav\n",
        "def loadFileFromWebDav(site, filepath, filename, login, password):\n",
        "  if os.path.isfile(filename):\n",
        "    return\n",
        "\n",
        "  netrc = os.environ['HOME'] + \"/.netrc\"\n",
        "\n",
        "  if not os.path.isfile(netrc): #Check if .netrc file for WebDav access exists\n",
        "    #!rm $netrc\n",
        "    netrcText = \"machine \" + site + \" login \" + login + \" password \" + password\n",
        "    !echo $netrcText > $netrc\n",
        "    !chmod 600 $netrc\n",
        "    !cat $netrc\n",
        "\n",
        "  result = !cadaver --version #Check if WebDav console client cadaver is installed\n",
        "  if \"command not found\" in result:\n",
        "    !sudo apt-get install cadaver\n",
        "\n",
        "  result = !cadaver --version\n",
        "  if not (\"command not found\" in result): #Create file to get the file from WebDav\n",
        "    #!rm .cadaverrc\n",
        "    cadaverrc = \"cd \" + filepath + \"\\r\\n\"\n",
        "    cadaverrc += \"get \" + filename + \"\\r\\n\"\n",
        "    cadaverrc += \"exit\\r\\n\"\n",
        "\n",
        "    file1 = open(\"cadaverrc\",\"w\")\n",
        "    file1.write(cadaverrc)\n",
        "    file1.close()\n",
        "    !cat \"cadaverrc\"\n",
        "\n",
        "    webdavurl = \"https://\" + site\n",
        "    !cadaver -r cadaverrc $webdavurl\n",
        "\n",
        "    if \".zip\" in filename: #Unpack file if it was compressed by zip\n",
        "      !unzip $filename"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4c5IrmteLlL"
      },
      "outputs": [],
      "source": [
        "# # загрузим необработанный файл с ответами в GoogleColab\n",
        "# # !rm .cadaverrc\n",
        "# !rm Otvety.txt\n",
        "# loadFileFromWebDav(\"webdav.yandex.ru\", \"Colab\", \"Otvety.txt\", \"pavelzrenie\", \"nuagieinlgexdsbi\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCmLN2uskC80"
      },
      "outputs": [],
      "source": [
        "# Выведем содержимое файла Otvety.txt на экран\n",
        "# f = 0\n",
        "# with codecs.open(\"Otvety.txt\", \"r\", \"utf-8\") as fin:\n",
        "#       # из исходного файла перебираем строки\n",
        "#       for line in fin:\n",
        "#         print(line)\n",
        "#         f += 1\n",
        "#         if f == 5:\n",
        "#           break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nu6XPTUcJ_rW",
        "outputId": "f6009471-5f7e-43d7-abda-06e6e9291876"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cd Colab\r\n",
            "get ProductsDataset.zip\r\n",
            "exit\n",
            "Downloading `/Colab/ProductsDataset.zip' to ProductsDataset.zip:\n",
            "Progress: [=============================>] 100.0% of 4398055 bytes succeeded.\n",
            "Connection to `webdav.yandex.ru' closed.\n",
            "Archive:  ProductsDataset.zip\n",
            "  inflating: ProductsDataset.csv     \n"
          ]
        }
      ],
      "source": [
        "# загрузим файл с описанием товаров\n",
        "!rm ProductsDataset.zip\n",
        "!rm ProductsDataset.csv\n",
        "loadFileFromWebDav(\"webdav.yandex.ru\", \"Colab\", \"ProductsDataset.zip\", \"pavelzrenie\", \"nuagieinlgexdsbi\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QClWkFMOe7BJ"
      },
      "outputs": [],
      "source": [
        "# Загрузим необходимые для работы модели файлы из GoogleDisk в Colab\n",
        "\n",
        "# В Colab импортируем все необходимые библиотеки\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Привяжем GoogleDisk в Colab\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Загрузим необходимые файлы в Colab\n",
        "\n",
        "# предобработанные для дальнейших манипуляций данные файла Otvety.txt\n",
        "# https://drive.google.com/file/d/1OSBU1WbgVrmuIRF9r3CCMpg01-NljG5C/view?usp=drive_link\n",
        "download = drive.CreateFile({\"id\": \"1OSBU1WbgVrmuIRF9r3CCMpg01-NljG5C\"})\n",
        "download.GetContentFile(\"prepared_answers.txt\")\n",
        "\n",
        "# векторизованные слова из файла Otvety.txt\n",
        "# https://drive.google.com/file/d/1-7F4CnTbLIKyD0xbTGAH0OSkKA-oYFcw/view?usp=sharing\n",
        "download = drive.CreateFile({\"id\": \"1-7F4CnTbLIKyD0xbTGAH0OSkKA-oYFcw\"})\n",
        "download.GetContentFile(\"model_w2v\")\n",
        "\n",
        "# индексированные вопросы для чата-болталки\n",
        "# https://drive.google.com/file/d/1St82kpoA9XLvQiiCahoPNVzovgIsdD63/view?usp=sharing\n",
        "download = drive.CreateFile({\"id\": \"1St82kpoA9XLvQiiCahoPNVzovgIsdD63\"})\n",
        "download.GetContentFile(\"speaker.ann\")\n",
        "\n",
        "# индексированные ответы для чата-болталки\n",
        "# https://drive.google.com/file/d/1-AFmyRO_dg1GtoByCgTP1_2NZfrDTOv9/view?usp=sharing\n",
        "download = drive.CreateFile({\"id\": \"1-AFmyRO_dg1GtoByCgTP1_2NZfrDTOv9\"})\n",
        "download.GetContentFile(\"index_map\")\n",
        "\n",
        "# данные для продуктового чат-бота ProductsDataset.zip\n",
        "# https://drive.google.com/file/d/17OdOPIH9d-T6d1Rb6F0IQuI7P9xoGWHi/view?usp=sharing\n",
        "download = drive.CreateFile({\"id\": \"17OdOPIH9d-T6d1Rb6F0IQuI7P9xoGWHi\"})\n",
        "download.GetContentFile(\"ProductsDataset.zip\")\n",
        "\n",
        "# данные для создания модели классификации запросов\n",
        "# https://drive.google.com/file/d/1-7EyqxjKyh7Y4TNBaxQ_mpAUR66TCZeZ/view?usp=drive_link\n",
        "download = drive.CreateFile({\"id\": \"1-7EyqxjKyh7Y4TNBaxQ_mpAUR66TCZeZ\"})\n",
        "download.GetContentFile(\"classification_dataset\")\n",
        "\n",
        "# сериализованная модель классификатора\n",
        "# https://drive.google.com/file/d/1S4GjYmXf9WEtGtt6HUFjTfJGufe5lR3R/view?usp=drive_link\n",
        "download = drive.CreateFile({\"id\": \"1S4GjYmXf9WEtGtt6HUFjTfJGufe5lR3R\"})\n",
        "download.GetContentFile(\"model_cs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "toygnT9cYwQ2"
      },
      "outputs": [],
      "source": [
        "# десериализуем полученные данные\n",
        "import pickle\n",
        "\n",
        "with open ('model_w2v', 'rb') as fp:\n",
        "    model = pickle.load(fp)\n",
        "\n",
        "with open('index_map','rb') as file:\n",
        "    index_map = pickle.load(file)\n",
        "\n",
        "with open ('model_cs', 'rb') as fp:\n",
        "    model_cs = pickle.load(fp)\n",
        "\n",
        "with open ('classification_dataset', 'rb') as fp:\n",
        "    classification_dataset = pickle.load(fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pWDR5f6-h-a",
        "outputId": "2bea81f1-5116-412c-c2cb-b5bb051b54ad"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# воссоздадим индескировааные данные о словах для чата-болталки\n",
        "index = AnnoyIndex(100, 'angular')\n",
        "index.load('speaker.ann')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ugMEtd-IEfn"
      },
      "source": [
        "# Создание функции для препроцессинга текста"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9361dcb7-b80b-489e-8d88-03bb8c2d838e"
      },
      "outputs": [],
      "source": [
        "# создадим экземпляр класса для лемматизации\n",
        "morpher = MorphAnalyzer()\n",
        "# создадим множество стоп-слов\n",
        "sw = set(get_stop_words(\"ru\"))\n",
        "# создадим множество со знаками пунктуации\n",
        "exclude = set(string.punctuation)\n",
        "\n",
        "def preprocess_txt(line):\n",
        "    \"\"\"Функция для предобработки текста\n",
        "\n",
        "    Args:\n",
        "        line (string): обрабатываемый текст\n",
        "\n",
        "    Returns:\n",
        "        string: Очищенный от знаков препинания и стоп-слов текст, лемматизированный\n",
        "                и приведенный к нормальной форме\n",
        "    \"\"\"\n",
        "    # очистка от знаков препинания\n",
        "    spls = \"\".join(i for i in line.strip() if i not in exclude).split()\n",
        "    # принимаем слово (обязательно в нижнем регистре) и возвращаем нормальную форму\n",
        "    spls = [morpher.parse(i.lower())[0].normal_form for i in spls]\n",
        "    # удаляем стоп-слова\n",
        "    spls = [i for i in spls if i not in sw and i != \"\"]\n",
        "    return spls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YwC32TTHTC-"
      },
      "source": [
        "# Данные \"болталки\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQue29rQAkP_"
      },
      "source": [
        "## Предобработаем данные болталки"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQ9wFPilTiAb"
      },
      "outputs": [],
      "source": [
        "# # вопрос не написан\n",
        "# question = None\n",
        "# # ответ не написан\n",
        "# written = False\n",
        "\n",
        "# # записываем в файл подготовленные ответы(prepared_answers.txt)\n",
        "# with codecs.open(\"prepared_answers.txt\",\"w\", \"utf-8\") as fout:\n",
        "#   # читаем файл Otvety.txt\n",
        "#     with codecs.open(\"Otvety.txt\", \"r\", \"utf-8\") as fin:\n",
        "#       # из исходного файла перебираем строки\n",
        "#         for line in tqdm(fin):\n",
        "#           # если строка начинается с ---\n",
        "#             if line.startswith(\"---\"):\n",
        "#                 # ответ не написан\n",
        "#                 written = False\n",
        "#                 # переходим к следующей строке\n",
        "#                 continue\n",
        "#             # если строка не начинается с \"---\", ответ не написан\n",
        "#             # и у нас есть ответ на вопрос\n",
        "#             if not written and question is not None:\n",
        "#                 # записываем строку в файл prepared_answers.txt текст: вопрос+tab+ответ\n",
        "#                 fout.write(question.replace(\"\\t\", \" \").strip() + \"\\t\" + line.replace(\"\\t\", \" \"))\n",
        "#                 # ответ написан\n",
        "#                 written = True\n",
        "#                 # вопрос не написан\n",
        "#                 question = None\n",
        "#                 # идем к следующей строке\n",
        "#                 continue\n",
        "#             # если строка не --- , ответ не написан\n",
        "#             if not written:\n",
        "#                 # значит это вопрос\n",
        "#                 question = line.strip()\n",
        "#                 # переходим к следующей строке\n",
        "#                 continue\n",
        "# # сохраним файл для дальнейшего использования\n",
        "# # index.save('prepared_answers.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2oc8e8UH2Yc"
      },
      "source": [
        "## Векторизация слов в данных \"болталки\" по методу Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4fbeff0-a2ca-4a5b-8adb-c6182abcfa96"
      },
      "outputs": [],
      "source": [
        "# # создадим список предложений\n",
        "# sentences = []\n",
        "# # создадим экземпляр класса для лемматизации\n",
        "# morpher = MorphAnalyzer()\n",
        "# # создадим множество стоп-слов\n",
        "# sw = set(get_stop_words(\"ru\"))\n",
        "# # создадим множество со знаками пунктуации\n",
        "# exclude = set(string.punctuation)\n",
        "# # счетчик количества обрабатываемых пар вопрос-ответ\n",
        "# c = 0\n",
        "\n",
        "# # открываем файл с необработанными данными \"болталки\"\n",
        "# with codecs.open(\"Otvety.txt\", \"r\", \"utf-8\") as fin:\n",
        "#     # используем визуализацию процесса итерации строк\n",
        "#     for line in tqdm(fin):\n",
        "#       # преодобработаем тексты вопросов-ответов\n",
        "#         spls = preprocess_txt(line)\n",
        "#         # запишем обработанные строки в список\n",
        "#         sentences.append(spls)\n",
        "#         c += 1\n",
        "#         # ограничим количество предобработанных строк до 500000\n",
        "#         if c > 500000:\n",
        "#             break\n",
        "\n",
        "# # Обучим модель word2vec на данных \"болталки\" для получения векторных представлений слов\n",
        "# # оставим в списке предложений те, в которых более 2 символов\n",
        "# sentences = [i for i in sentences if len(i) > 2]\n",
        "# # размер векторов слов = 100,\n",
        "# # игнорировать слова с частотой менее 1,\n",
        "# # максимальное расстояние между текущим и предсказанным словом в предложении = 5\n",
        "# model = Word2Vec(sentences=sentences, vector_size=100, min_count=1, window=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amhu6767-CUG"
      },
      "source": [
        "## Создание индекса вопросов \"болталки\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca232edd-5811-4692-bcd7-c075c89fb4dd"
      },
      "source": [
        "Теперь нам нужно сложить в индекс все вопросы.  \n",
        "Для создания индекса используем библиотеку **annoy**.   \n",
        "Проходимся по всем вопросам, здесь нам пригодится предобработанный файл и модель word2Vec.   \n",
        "Для создания индекса считаем, что вектор предложения - сумма word2vecов слов, которые входят в него (усредненная)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrxWEvXsKCyd"
      },
      "outputs": [],
      "source": [
        "# # Количество признаков (размерностей) хранимого векторa = 100\n",
        "# # Метрика расстояния между векторами («угловой»)\n",
        "# index = annoy.AnnoyIndex(100 ,'angular')\n",
        "\n",
        "# # словарь, в котором будет находится индекс\n",
        "# index_map = {}\n",
        "# # счетчик элементов индекса\n",
        "# counter = 0\n",
        "\n",
        "# # для создания индекса используем предобработанный в формат вопрос-ответ исходный файл\n",
        "# with codecs.open(\"prepared_answers.txt\", \"r\", \"utf-8\") as f:\n",
        "#     # проходим по строкам предобработанного файла\n",
        "#     for line in tqdm(f):\n",
        "#       # счетчик количества слов с вектором в вопросе\n",
        "#         n_w2v = 0\n",
        "#         # делим строку предобработанного файла на вопрос\n",
        "#         # и ответ по символу табуляции, проставленному при обработке\n",
        "#         spls = line.split(\"\\t\")\n",
        "#         # по очереди заполняем словарь(номер индекса-ответ на вопрос)\n",
        "#         index_map[counter] = spls[1]\n",
        "#         # производим предобработку вопроса\n",
        "#         question = preprocess_txt(spls[0])\n",
        "#         # создаем нулевую матрицу(100,1) из 100 элементов\n",
        "#         vector = np.zeros(100)\n",
        "#         # для каждого слова в предобработанном вопросе\n",
        "#         for word in question:\n",
        "#           # если даннное слово присутствует в списке созданных векторов\n",
        "#             if word in model.wv:\n",
        "#               # суммируем полученные вектора слов\n",
        "#                 vector += model.wv[word]\n",
        "#               # увеличиваем счетчик количества слов в вопросе\n",
        "#                 n_w2v += 1\n",
        "#       # если в вопросе есть слова\n",
        "#         if n_w2v > 0:\n",
        "#           # вычисляем средний вектор\n",
        "#             vector = vector / n_w2v\n",
        "#       # записываем в индекс номер индекса и средний вектор входящих в\n",
        "#       # соответсвующий данному номеру вопрос слов\n",
        "#         index.add_item(counter, vector)\n",
        "#       # увеличиваем номер индекса\n",
        "#         counter += 1\n",
        "# # количество деревьев в лесу(чем больше, тем точнее)\n",
        "# index.build(10)\n",
        "# # сохраним полученный индекс в файл для дальнейшего использования\n",
        "# # index.save('speaker.ann')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxPfNqkRNRuo"
      },
      "source": [
        "## Поиск ответа в случае, если запрос классифицирован как вопрос \"болталки\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59a5984e-3fb9-4c87-bf20-d95bfdc8f80c"
      },
      "outputs": [],
      "source": [
        "def find_answer_ovety(question):\n",
        "\n",
        "   \"\"\"Функция реализует метод, который получает на вход вопрос и находит ответ к нему.\n",
        "      Для этого происходит препроцессинг вопроса, далее находится ближайший вопрос и\n",
        "      происходит выбор ответа на ближайший вопрос.\n",
        "\n",
        "    Args:\n",
        "        question (string): заданный вопрос\n",
        "\n",
        "    Returns:\n",
        "        string: найденный ответ\n",
        "    \"\"\"\n",
        "    # готовим для обработки с помощью препроцессинга заданный вопрос\n",
        "   preprocessed_question = preprocess_txt(question)\n",
        "    # счетчик слов в вопросе\n",
        "   n_w2v = 0\n",
        "    # создаем вектор\n",
        "   vector = np.zeros(100)\n",
        "    # определяем сумму векторов слов входящих в вопрос\n",
        "    # и количество слов входящих в вопрос\n",
        "   for word in preprocessed_question:\n",
        "       if word in model.wv:\n",
        "          vector += model.wv[word]\n",
        "          n_w2v += 1\n",
        "  # определяем средний вектор соответствующий заданному вопросу\n",
        "   if n_w2v > 0:\n",
        "      vector = vector / n_w2v\n",
        "  # ищем 1 ближайший вектор заданного вопроса и вопроса из индекса\n",
        "   answer_index = index.get_nns_by_vector(vector, 1)\n",
        "    # возвращаем ответ из наиболее близкого заданному вопроса\n",
        "   return index_map[answer_index[0]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0BBG01Xjq56",
        "outputId": "9846d612-41f7-4bc2-cbc7-1dd29f457c3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "танки онлайн, мир танков. \n",
            "\n"
          ]
        }
      ],
      "source": [
        "assert(not find_answer_ovety('Где ключи от танка').startswith('5'))\n",
        "gg = find_answer_ovety('Где ключи от танка')\n",
        "print(gg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dC-LpbZG7Ef"
      },
      "source": [
        "# Данные о продуктах"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRiqKFydEK1h"
      },
      "source": [
        "## Предобработаем данные о продуктах"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6coCjP7DaKU"
      },
      "outputs": [],
      "source": [
        "# подготовим данные с запросами продуктового чата\n",
        "products = pd.read_csv('/content/ProductsDataset.zip')\n",
        "products = products[['title','product_id']]\n",
        "products.dropna(inplace = True)\n",
        "products.reset_index(drop= True, inplace = True)\n",
        "products['content_type'] = np.ones(len(products), dtype = int)\n",
        "# препроцессинг текста\n",
        "products['preprocess_txt'] = products['title'].apply(lambda x: preprocess_txt(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiYWOIZcG4Mf"
      },
      "source": [
        "## Векторизируем слова по методу Word2Vec в данных о продуктах\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ds8ZlzBiQhUY"
      },
      "outputs": [],
      "source": [
        "## Обучим модель word2vec на названиях продуктов\n",
        "# оставим в списке предложений те, в которых более 2 символов\n",
        "sentences = products['preprocess_txt'].tolist()\n",
        "# создадим модель для получения векторных представлений слов\n",
        "# размер векторов слов = 30,\n",
        "# игнорировать слова с частотой менее 1,\n",
        "# максимальное расстояние между текущим и предсказанным словом в предложении = 3\n",
        "model_product = Word2Vec(sentences=sentences, vector_size=10, min_count=1, window=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjVAZBIfEU_6"
      },
      "source": [
        "## Создание индекса вопросов данных о продуктах"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c81aad5c-9a87-4962-8707-376cad951dcb",
        "outputId": "41420e06-a4ec-4b3e-f955-faa859148aa3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# Количество признаков (размерностей) хранимого векторa = 30\n",
        "# Метрика расстояния между векторами («угловой»)\n",
        "index_product = annoy.AnnoyIndex(10 ,'angular')\n",
        "\n",
        "# словарь, в котором будет находится индекс\n",
        "index_product_map = {}\n",
        "# счетчик элементов индекса\n",
        "counter = 0\n",
        "\n",
        "# для создания индекса используем предобработанный в формат вопрос-ответ исходный файл\n",
        "while counter < len(products):\n",
        "    # проходим по строкам предобработанного файла\n",
        "  # счетчик количества слов с вектором в вопросе\n",
        "  n_w2v = 0\n",
        "        # по очереди заполняем словарь(номер индекса-идентификационный номер продукта)\n",
        "  index_product_map[counter] = products.loc[counter,'product_id']\n",
        "        # создаем нулевую матрицу(30,1) из 30 элементов\n",
        "  vector = np.zeros(10)\n",
        "        # для каждого слова в предобработанном вопросе\n",
        "  for word in products.loc[counter,'preprocess_txt']:\n",
        "    # если даннное слово присутствует в списке созданных векторов\n",
        "    if word in model_product.wv:\n",
        "      # суммируем полученные вектора слов\n",
        "      vector += model_product.wv[word]\n",
        "      # увеличиваем счетчик количества слов в вопросе\n",
        "      n_w2v += 1\n",
        "      # если в вопросе есть слова\n",
        "  if n_w2v > 0:\n",
        "        # вычисляем средний вектор\n",
        "    vector = vector / n_w2v\n",
        "      # записываем в индекс номер и средний вектор\n",
        "  index_product.add_item(counter, vector)\n",
        "      # увеличиваем счетчик индекса\n",
        "  counter += 1\n",
        "# количество деревьев в лесу\n",
        "index_product.build(30)\n",
        "\n",
        "# # сохраним полученные данные в файлы для дальнейшего использования\n",
        "# with open('index_product_map', 'wb') as fp:\n",
        "#     pickle.dump(index_product_map, fp)\n",
        "\n",
        "# index_product.save('index_product.ann')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2uuVxU6JGN0"
      },
      "source": [
        "## Поиск ответа в случае, если запрос классифицирован как вопрос о продукте"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qftW_d1WmJkk"
      },
      "outputs": [],
      "source": [
        "def find_answer_products(question):\n",
        "   \"\"\"Функция реализует метод, который получает на вход вопрос и находит ответ к нему.\n",
        "      Для этого вопрос  препроцессится, далее находится ближайший вопрос и\n",
        "      происходит выбор ответа на ближайший вопрос.\n",
        "\n",
        "    Args:\n",
        "        question (string): заданный вопрос\n",
        "\n",
        "    Returns:\n",
        "        string: найденный ответ\n",
        "    \"\"\"\n",
        "    # готовим для обработки с помощью препроцессинга заданный вопрос\n",
        "   preprocessed_question = preprocess_txt(question)\n",
        "  #  print(preprocessed_question)\n",
        "    # счетчик слов в вопросе\n",
        "   n_w2v = 0\n",
        "    # создаем вектор\n",
        "   vector = np.zeros(10)\n",
        "    # определяем сумму векторов слов входящих в вопрос\n",
        "    # и количество слов входящих в вопрос\n",
        "   for word in preprocessed_question:\n",
        "    # print(word)\n",
        "    if word in model_product.wv:\n",
        "      vector += model_product.wv[word]\n",
        "      n_w2v += 1\n",
        "    # определяем средний вектор соответствующий заданному вопросу\n",
        "   if n_w2v > 0:\n",
        "    vector = vector / n_w2v\n",
        "    # ищем 1 ближайший вектор заданного вопроса и вопроса из индекса\n",
        "    answer_index = index_product.get_nns_by_vector(vector, 5, include_distances=False)\n",
        "   # возвращаем ответ из наиболее близкого заданному вопроса\n",
        "   title = products['title'].loc[answer_index[0]]\n",
        "   return f'{index_product_map[answer_index[0]]} {title}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHb6C5S--SdC",
        "outputId": "a1f91301-3beb-4059-b74d-b3a7e193b98b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "58e3cfe6132ca50e053f5f82 Юбка детская ORBY\n"
          ]
        }
      ],
      "source": [
        "assert(find_answer_products('Юбка детская ORBY').startswith('58e3cfe6132ca50e053f5f82'))\n",
        "ff = find_answer_products('Юбка детская ORBY')\n",
        "print(ff)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56zD0YvZHcf4"
      },
      "source": [
        "# Создание классификатора запросов чат-бота"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4-eFFFurYZZ"
      },
      "outputs": [],
      "source": [
        "# # создадим файл для обучения модели классификации запросов\n",
        "\n",
        "# # подготовим данные с запросами вопросов чата-болталки\n",
        "# dict_questions = {}\n",
        "# with codecs.open(\"prepared_answers.txt\", \"r\", \"utf-8\") as fin:\n",
        "#       # из исходного файла перебираем строки\n",
        "#       for line in fin:\n",
        "#         dict_questions[line.split(\"\\t\")[0]] = 0\n",
        "\n",
        "# content_dataset = pd.DataFrame(list(dict_questions.items()), columns = ['title','content_type'])\n",
        "# content_dataset['preprocess_txt'] = content_dataset['title'][0:36000].apply(lambda x: preprocess_txt(x))\n",
        "# content_dataset.dropna(inplace = True)\n",
        "\n",
        "# # объединение данных в файл для обучения модели\n",
        "# classification_dataset = pd.concat([content_dataset, products[['title','content_type','preprocess_txt']]])\n",
        "\n",
        "# # сохранение полученного файла на GoogleDisk\n",
        "# # with open('classification_dataset', 'wb') as fp:\n",
        "# #     pickle.dump(classification_dataset, fp)\n",
        "# # !cp /content/classification_dataset /content/drive/MyDrive/Colab/Векторизация\\ и\\ классификация\\ текста"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFJQXCyk0afg"
      },
      "source": [
        "Данные для создания классификатора подготовлены"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naWtP6skzMbe"
      },
      "source": [
        "После подготовки данных для обучения модели классификации запросов проверим данные на сбалансированность классов и разделим выборку на тренировочную и валидационную части"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cav179WnqnNn",
        "outputId": "67591162-5258-4673-85be-acf9eb205e91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    51.77\n",
            "1    48.23\n",
            "Name: content_type, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# проверка на сбалансированность классов\n",
        "print(round(classification_dataset['content_type'].value_counts(normalize = True)*100,2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0_HKLUr0ELA"
      },
      "source": [
        "Вывод: Данные сбалансированы"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7Y3BIpqqnNp",
        "outputId": "eabbdf2d-16be-4bdc-b03c-7ea23e24d3a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99     10740\n",
            "           1       1.00      0.98      0.99     10121\n",
            "\n",
            "    accuracy                           0.99     20861\n",
            "   macro avg       0.99      0.99      0.99     20861\n",
            "weighted avg       0.99      0.99      0.99     20861\n",
            "\n"
          ]
        }
      ],
      "source": [
        "X_train, X_valid, y_train, y_valid = train_test_split(classification_dataset['title'],\n",
        "                                                            classification_dataset['content_type'],\n",
        "                                                            test_size=0.3,\n",
        "                                                            random_state=42\n",
        "                                                            )\n",
        "# print('Train:\\n', y_train.value_counts(normalize=True), sep='')\n",
        "# print('Valid:\\n', y_valid.value_counts(normalize=True), sep='')\n",
        "\n",
        "# создадим и обучим объект векторизации TfidfVectorizer\n",
        "vectorizer  = TfidfVectorizer()\n",
        "values = vectorizer.fit(X_train)\n",
        "\n",
        "# конвертируем тексты(обучающий и тестовый) в векторы tf-idf\n",
        "X_train = values.transform(X_train)\n",
        "X_valid = values.transform(X_valid)\n",
        "\n",
        "# СОГЛАСНО УСЛОВИЯМ ЗАДАНИЯ МОДЕЛЬ ДОЛЖНА ЗАГРУЖАТЬСЯ ИЗ pkl-файла,\n",
        "# ЭТО ПРОИСХОДИТ В НАЧАЛЕ РАБОТЫ НОУТБУКА\n",
        "# # создадим экземпляр класса LogisticRegression()\n",
        "# model_cs = LogisticRegression()\n",
        "# # обучаем модель на тренировочной выборке\n",
        "# model_cs.fit(X_train, y_train)\n",
        "# делаем предсказание для валидационной выборки\n",
        "y_valid_pred = model_cs.predict(X_valid)\n",
        "# выводим значения метрик\n",
        "print(metrics.classification_report(y_valid, y_valid_pred))\n",
        "\n",
        "def classification(request):\n",
        "  \"\"\"Функция реализует классификацию запросов на продуктовые и \"болталку\"\n",
        "\n",
        "    Args:\n",
        "        request (str): Запрос\n",
        "\n",
        "    Returns:\n",
        "        int: 1 если продуктовый запрос, 0 если \"болталка\"\n",
        "    \"\"\"\n",
        "  vec = vectorizer.transform([request])\n",
        "  if model_cs.predict(vec)[0] == 1:\n",
        "    return 1\n",
        "  else: 0\n",
        "\n",
        "# # Сериализация модели и перенос в GoogleDrive\n",
        "# import pickle\n",
        "# with open('model_cs', 'wb') as fp:\n",
        "#     pickle.dump(model_cs, fp)\n",
        "# !cp /content/model_cs /content/drive/MyDrive/Colab/Векторизация\\ и\\ классификация\\ текста"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Сериализация модели и перенос в GoogleDrive\n",
        "# import pickle\n",
        "# with open('model_cs', 'wb') as fp:\n",
        "#     pickle.dump(model_cs, fp)\n",
        "# !cp /content/model_cs /content/drive/MyDrive/Colab/Векторизация\\ и\\ классификация\\ текста"
      ],
      "metadata": {
        "id": "KrQkxSclwebV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sssoe-wT3Ith"
      },
      "source": [
        "Вывод: метрики показывающие качество классификации на достаточно высоком уровне, оптимизация не требуется."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0keAbRV9QXNl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f65b7090-d5ab-4de0-d430-3b1f4172db4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Запрос: Где живет кот. Ответ чат-бота: Question\n",
            "Запрос: Ботильоны. Ответ чат-бота: Product\n",
            "Запрос: Юбка детская ORBY. Ответ чат-бота: Product\n",
            "Запрос: Где ключи от танка. Ответ чат-бота: Question\n"
          ]
        }
      ],
      "source": [
        "# проверка\n",
        "requests = ['Где живет кот', 'Ботильоны', 'Юбка детская ORBY', 'Где ключи от танка']\n",
        "for request in requests:\n",
        "  response = 'Product' if classification(request) == 1 else 'Question'\n",
        "  print(f'Запрос: {request}. Ответ чат-бота: {response}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLxd4QCgPFj3"
      },
      "source": [
        "# Создание чат-бота"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZ9hfjSWNbDW"
      },
      "outputs": [],
      "source": [
        "def get_answer(request):\n",
        "  \"\"\"Функция возвращает ответ на вопрос в зависимости от его\n",
        "     типа(продуктовый или \"болталка\")\n",
        "\n",
        "    Args:\n",
        "        request (str): Вопрос чат-боту\n",
        "\n",
        "    Returns:\n",
        "        str: Ответ чат-бота\n",
        "    \"\"\"\n",
        "  if classification(request) == 1:\n",
        "    return find_answer_products(request)\n",
        "  return find_answer_ovety(request)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBTn9-p1PrQK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36060b83-b819-4a01-f3b7-7dc0d5a41a12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Запрос: Где живет кот. Ответ чат-бота: у Сидоровой козы.... \n",
            "\n",
            "Запрос: Ботильоны. Ответ чат-бота: 5667531b2b7f8d127d838c34 Ботильоны\n",
            "Запрос: Юбка детская ORBY. Ответ чат-бота: 58e3cfe6132ca50e053f5f82 Юбка детская ORBY\n",
            "Запрос: Где ключи от танка. Ответ чат-бота: танки онлайн, мир танков. \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# проверка\n",
        "requests = ['Где живет кот', 'Ботильоны', 'Юбка детская ORBY', 'Где ключи от танка']\n",
        "for request in requests:\n",
        "  # response = 'Product' if classification(request) == 1 else 'Question'\n",
        "  print(f'Запрос: {request}. Ответ чат-бота: {get_answer(request)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17yYU8mTOj7_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a72b4607-d7ed-475f-c59e-6e24e36dbe3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "# Проверка\n",
        "assert(get_answer('Юбка детская ORBY').startswith('58e3cfe6132ca50e053f5f82'))\n",
        "print('True')\n",
        "assert(not get_answer('Где ключи от танка').startswith('5'))\n",
        "print('True')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Результат: Чат-бот классифицирует запросы и выводит релевантные ответы."
      ],
      "metadata": {
        "id": "3WFCl-mzDEFh"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}